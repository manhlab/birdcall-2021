{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "v5 \u001dsed_effnet0_new_datasetv2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "intended-dealing",
        "Io4OdQSqmHZY"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4309.839023,
      "end_time": "2021-04-05T15:56:55.705989",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-04-05T14:45:05.866966",
      "version": "2.3.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc23ca52e64940ecb3b8b57e7af9116f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b98708c585bf41e9be5d10316d1f1148",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d25bdf9d7585445c82653283ca057773",
              "IPY_MODEL_9b048b2d16e84e03a7eebcfbeef704b8"
            ]
          }
        },
        "b98708c585bf41e9be5d10316d1f1148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d25bdf9d7585445c82653283ca057773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68cad89daa2e4b998f4ff744e9a0e1f9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1258,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1258,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3161935d7c4441f5aa61e8cc5523a8d9"
          }
        },
        "9b048b2d16e84e03a7eebcfbeef704b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_92ec5eabc63f483084d9c089051f6e18",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1256/1258 [10:32&lt;00:00,  2.13it/s, f1=0.008, loss=0.039124, lrap=0.091, prec=0.111, rec=0.005]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdabb104de4f43dc8a78913792b64324"
          }
        },
        "68cad89daa2e4b998f4ff744e9a0e1f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3161935d7c4441f5aa61e8cc5523a8d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92ec5eabc63f483084d9c089051f6e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdabb104de4f43dc8a78913792b64324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smaller-radio"
      },
      "source": [
        "## About this notebook\n",
        "\n",
        "In this notebook, I will show the way to train the model used here:\n",
        "https://www.kaggle.com/hidehisaarai1213/pytorch-inference-birdclef2021-starter\n",
        "\n",
        "Note that by default this notebook will only train the model one epoch, but [the weight](https://www.kaggle.com/hidehisaarai1213/birdclef2021-effnetb0-starter-weight) I used was obtained after 31epochs of training.\n"
      ],
      "id": "smaller-radio"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "horizontal-denial"
      },
      "source": [
        "## Dependencies"
      ],
      "id": "horizontal-denial"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IChChsY4XhJr",
        "outputId": "bfe680f2-2f34-4929-e758-4a1755836572"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "IChChsY4XhJr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 24 15:00:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm5iO26sXa-O",
        "outputId": "e2d279cb-72a5-4123-9d45-c9951d706e35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        " \n",
        "from shutil import copyfile\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp \"/content/gdrive/My Drive/Kaggle/kaggle.json\" ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!rm -rf audio_images\n",
        "!kaggle datasets download --unzip -d doanquanvietnamca/birdclef20ssed1\n",
        "!kaggle datasets download --unzip -d doanquanvietnamca/birdclef20ssed2\n",
        "!kaggle datasets download --unzip -d doanquanvietnamca/birdclef20ssed3\n",
        "!kaggle datasets download --unzip -d doanquanvietnamca/birdclef20ssed4\n",
        "# !kaggle datasets download --unzip -d truonghoang/birdclef-short-audio-extract-energy-1\n",
        "# !kaggle datasets download --unzip -d truonghoang/birdclef-short-audio-extract-energy-2\n",
        "# !kaggle datasets download --unzip -d truonghoang/birdclef-short-audio-extract-energy-3\n",
        "# !kaggle datasets download --unzip -d truonghoang/birdclef-short-audio-extract-energy-4"
      ],
      "id": "Pm5iO26sXa-O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e7/3bac01547d2ed3d308ac92a0878fbdb0ed0f3d41fb1906c319ccbba1bfbc/kaggle-1.5.12.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-cp37-none-any.whl size=73053 sha256=b7f6c8ec9d3561466f7bd58a824f1a1595654cd7418deb2f9e8047c69933f148\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n",
            "Downloading birdclef20ssed1.zip to /content\n",
            "100% 9.80G/9.81G [03:04<00:00, 58.3MB/s]\n",
            "100% 9.81G/9.81G [03:04<00:00, 57.0MB/s]\n",
            "Downloading birdclef20ssed2.zip to /content\n",
            "100% 10.2G/10.2G [03:14<00:00, 59.6MB/s]\n",
            "100% 10.2G/10.2G [03:14<00:00, 56.5MB/s]\n",
            "Downloading birdclef20ssed3.zip to /content\n",
            "100% 9.24G/9.27G [02:47<00:00, 30.8MB/s]\n",
            "100% 9.27G/9.27G [02:47<00:00, 59.4MB/s]\n",
            "Downloading birdclef20ssed4.zip to /content\n",
            "100% 9.49G/9.51G [02:46<00:01, 21.7MB/s]\n",
            "100% 9.51G/9.51G [02:46<00:00, 61.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk81ufT9Kqr8",
        "outputId": "229d2e33-d177-4f07-a8a0-4ce79ca9f509"
      },
      "source": [
        "%cd /content/\n",
        "!rm -rf birdcall-2021\n",
        "!git clone https://manhlab:Sehocctf2019@github.com/manhlab/birdcall-2021.git\n",
        "!pip install -r /content/birdcall-2021/requirements.txt"
      ],
      "id": "Hk81ufT9Kqr8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'birdcall-2021'...\n",
            "remote: Enumerating objects: 489, done.\u001b[K\n",
            "remote: Counting objects: 100% (489/489), done.\u001b[K\n",
            "remote: Compressing objects: 100% (283/283), done.\u001b[K\n",
            "remote: Total 489 (delta 301), reused 373 (delta 199), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (489/489), 2.57 MiB | 7.09 MiB/s, done.\n",
            "Resolving deltas: 100% (301/301), done.\n",
            "Collecting torchlibrosa\n",
            "  Downloading https://files.pythonhosted.org/packages/52/a4/9bf7c8c24a828af8fa33593f745cc709a6bfa7fa893114df0c29e367e124/torchlibrosa-0.0.9-py2.py3-none-any.whl\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a0/dd40b50aebf0028054b6b35062948da01123d7be38d08b6b1e5435df6363/efficientnet_pytorch-0.7.1.tar.gz\n",
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/08/1ccaf8d516935666b7fa5f6aaddf157c66208ea0c93bb847ae09f166354f/timm-0.4.9-py3-none-any.whl (346kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 24.9MB/s \n",
            "\u001b[?25hCollecting fvcore\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/82/833257ef5192421c667f47b575bd90a9458949d1dee87fc3e84405d0300a/fvcore-0.1.5.post20210518.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hCollecting resnest\n",
            "  Downloading https://files.pythonhosted.org/packages/74/ab/bdd1a6ae83f13a268c736c7be2f14207f836711e22daa64d46b57262b66b/resnest-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch->-r /content/birdcall-2021/requirements.txt (line 2)) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm->-r /content/birdcall-2021/requirements.txt (line 3)) (0.9.1+cu101)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 33.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->-r /content/birdcall-2021/requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->-r /content/birdcall-2021/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->-r /content/birdcall-2021/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->-r /content/birdcall-2021/requirements.txt (line 4)) (0.8.9)\n",
            "Collecting iopath>=0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/21/d0/22104caed16fa41382702fed959f4a9b088b2f905e7a82e4483180a2ec2a/iopath-0.1.8-py3-none-any.whl\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from resnest->-r /content/birdcall-2021/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from resnest->-r /content/birdcall-2021/requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (0.10.3.post1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (2.1.9)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch->-r /content/birdcall-2021/requirements.txt (line 2)) (3.7.4.3)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->resnest->-r /content/birdcall-2021/requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->resnest->-r /content/birdcall-2021/requirements.txt (line 5)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->resnest->-r /content/birdcall-2021/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->resnest->-r /content/birdcall-2021/requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (56.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (1.14.5)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (20.9)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (2.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa>=0.6.0->torchlibrosa->-r /content/birdcall-2021/requirements.txt (line 1)) (2.4.7)\n",
            "Building wheels for collected packages: efficientnet-pytorch, fvcore\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-cp37-none-any.whl size=16443 sha256=280205d115fac91356b60b767b56e86d74d071fb9a68ba9fc23d259b2917a33b\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/27/aa/c46d23c4e8cc72d41283862b1437e0b3ad318417e8ed7d5921\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20210518-cp37-none-any.whl size=60362 sha256=8dbe5298ba66cce09532e632f14793b9e1ab0ef5b647e428923f5257e36f7c2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/0e/15/f134f13f031ac5f3973892c54513a5df360f92749fef15e7bd\n",
            "Successfully built efficientnet-pytorch fvcore\n",
            "Installing collected packages: torchlibrosa, efficientnet-pytorch, timm, pyyaml, yacs, portalocker, iopath, fvcore, nose, resnest\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed efficientnet-pytorch-0.7.1 fvcore-0.1.5.post20210518 iopath-0.1.8 nose-1.3.7 portalocker-2.3.0 pyyaml-5.4.1 resnest-0.0.5 timm-0.4.9 torchlibrosa-0.0.9 yacs-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ8vmp9S-pPO",
        "outputId": "c158e824-c56b-4145-f244-e0715fe0b88c"
      },
      "source": [
        "!mkdir /content/bird_background\n",
        "!kaggle datasets download --unzip -d theoviel/bird-backgrounds -op /content/bird_background"
      ],
      "id": "JJ8vmp9S-pPO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading bird-backgrounds.zip to /content/bird_background\n",
            " 82% 41.0M/50.0M [00:00<00:00, 31.3MB/s]\n",
            "100% 50.0M/50.0M [00:00<00:00, 52.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uwUpdEXwJe6",
        "outputId": "fa375fc2-6237-4a05-866c-b5dd21f22415"
      },
      "source": [
        "!kaggle datasets download --unzip -d qitvision/resnesttorchhubmodels -op /content\n",
        "!cp /content/resnest50-528c19ca.pth /root/.cache/torch/hub/checkpoints/resnest50-528c19ca.pth"
      ],
      "id": "9uwUpdEXwJe6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading resnesttorchhubmodels.zip to /content\n",
            " 91% 89.0M/97.7M [00:01<00:00, 35.3MB/s]\n",
            "100% 97.7M/97.7M [00:01<00:00, 52.5MB/s]\n",
            "cp: cannot create regular file '/root/.cache/torch/hub/checkpoints/resnest50-528c19ca.pth': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVsrSTpZDPI3",
        "outputId": "b0911c95-bacb-4084-c71f-d87558dc03ed"
      },
      "source": [
        "!python3 /content/birdcall-2021/cache_background.py"
      ],
      "id": "VVsrSTpZDPI3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KfQdZN_kXHI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579,
          "referenced_widgets": [
            "bc23ca52e64940ecb3b8b57e7af9116f",
            "b98708c585bf41e9be5d10316d1f1148",
            "d25bdf9d7585445c82653283ca057773",
            "9b048b2d16e84e03a7eebcfbeef704b8",
            "68cad89daa2e4b998f4ff744e9a0e1f9",
            "3161935d7c4441f5aa61e8cc5523a8d9",
            "92ec5eabc63f483084d9c089051f6e18",
            "cdabb104de4f43dc8a78913792b64324",
            "b629a00d612844449df2ba355fea8052"
          ]
        },
        "outputId": "ae3d0ab1-60f2-4d49-c96a-94bf0af4ffa7"
      },
      "source": [
        "%cd /content/birdcall-2021\n",
        "import sys\n",
        "from importlib import reload\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from glob import glob \n",
        "import torch\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "from  torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "from sed.model import ResNestSED, EfficientNetSED, TimmSED\n",
        "from sed.criterion import *\n",
        "from sed.config import CFG\n",
        "from sed.utils import *\n",
        "import librosa\n",
        "\n",
        "CFG.epochs = 50\n",
        "NMELS = 128\n",
        "CFG.batch_size = 40\n",
        "CFG.folds = [1]\n",
        "\n",
        "\n",
        "class BirdClefDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        meta,\n",
        "        sr=CFG.sr,\n",
        "        is_train=True,\n",
        "        num_classes=CFG.num_classes,\n",
        "        duration=CFG.duration,\n",
        "        background_audio=None,\n",
        "    ):\n",
        "        self.meta = meta.copy().reset_index(drop=True)\n",
        "        self.sr = sr\n",
        "        self.is_train = is_train\n",
        "        self.num_classes = num_classes\n",
        "        self.duration = duration\n",
        "        self.audio_length = self.duration * self.sr\n",
        "        self.background_audio = background_audio\n",
        "        self.n_mels = 128\n",
        "        self.len_chack = 626\n",
        "        self.stop_border = (\n",
        "            0.3  # Probability of stopping mixing | Вероятность прервать смешивание\n",
        "        )\n",
        "        self.level_noise = 0.05  # level noise | Уровень шума\n",
        "        self.div_coef = (\n",
        "            100  # signal amplification during mixing | Усиления сигнала при смешивании\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.meta.iloc[idx]\n",
        "        # imagesx = np.load(row.impath)\n",
        "        ebird_code = row[\"primary_label\"]\n",
        "        secondary_label = row[\"secondary_labels\"]\n",
        "        melspecs = np.load(row[\"impath\"], allow_pickle=True)\n",
        "        t_pobs = melspecs.item().get(\"probs\")\n",
        "        try:\n",
        "            images = melspecs.item().get(\"images\")[\n",
        "                np.random.choice(len(t_pobs), size=1, p=t_pobs)[0]\n",
        "            ]\n",
        "        except:\n",
        "            images = melspecs.item().get(\"images\")[np.random.choice(len(t_pobs))]\n",
        "        t = np.zeros(self.num_classes, dtype=np.float32)  # Label smoothing\n",
        "        t[CFG.target_columns.index(ebird_code)] = 1.0\n",
        "        \n",
        "        if self.is_train:\n",
        "            train_len_chack = random.randint(self.len_chack-48, self.len_chack+52)\n",
        "            if self.len_chack>train_len_chack:\n",
        "                start = random.randint(0, self.len_chack - train_len_chack - 1)\n",
        "                images = images[:, start : start + random.randint(train_len_chack-48, train_len_chack)]\n",
        "            else:\n",
        "                len_zero = random.randint(0, train_len_chack-self.len_chack)\n",
        "                images = np.concatenate((np.zeros((self.n_mels,len_zero)),images), axis=1)\n",
        "            images = np.concatenate((np.zeros((self.n_mels,train_len_chack - images.shape[1])),images), axis=1)\n",
        "            if random.random() < 0.9:\n",
        "                images = images + (\n",
        "                    np.random.sample((self.n_mels, train_len_chack)).astype(np.float32)\n",
        "                    + 9\n",
        "                ) * images.mean() * self.level_noise * (np.random.sample() + 0.3)\n",
        "\n",
        "            # Add pink noise | Добавить розовый шум\n",
        "            if random.random() < 0.9:\n",
        "                r = random.randint(1, self.n_mels)\n",
        "                pink_noise = np.array(\n",
        "                    [np.concatenate((1 - np.arange(r) / r, np.zeros(self.n_mels - r)))]\n",
        "                ).T\n",
        "                images = images + (\n",
        "                    np.random.sample((self.n_mels, train_len_chack)).astype(np.float32)\n",
        "                    + 9\n",
        "                ) * 2 * images.mean() * self.level_noise * (np.random.sample() + 0.3)\n",
        "\n",
        "            # Add bandpass noise | Добавить полосовой шум\n",
        "            if random.random() < 0.9:\n",
        "                a = random.randint(0, self.n_mels // 2)\n",
        "                b = random.randint(a + 20, self.n_mels)\n",
        "                images[a:b, :] = images[a:b, :] + (\n",
        "                    np.random.sample((b - a, train_len_chack)).astype(np.float32) + 9\n",
        "                ) * 0.05 * images.mean() * self.level_noise * (np.random.sample() + 0.3)\n",
        "\n",
        "            # Lower the upper frequencies | Понизить верхние частоты\n",
        "            if random.random() < 0.5:\n",
        "                images = images - images.min()\n",
        "                r = random.randint(self.n_mels // 2, self.n_mels)\n",
        "                x = random.random() / 2\n",
        "                pink_noise = np.array(\n",
        "                    [\n",
        "                        np.concatenate(\n",
        "                            (\n",
        "                                1 - np.arange(r) * x / r,\n",
        "                                np.zeros(self.n_mels - r) - x + 1,\n",
        "                            )\n",
        "                        )\n",
        "                    ]\n",
        "                ).T\n",
        "                images = images * pink_noise\n",
        "                images = images / (images.max() + 0.0000001)\n",
        "\n",
        "            # if random.random()<0.1:\n",
        "            #         w = np.random.uniform(0.2, 0.5)\n",
        "            #         images = (images + w*imagesx[np.random.choice(len(imagesx))])/(1+w)\n",
        "\n",
        "            if random.random()<0.5:\n",
        "                    k = np.random.uniform(0.0, 0.7)\n",
        "                    h = np.random.uniform(k, k+0.3)\n",
        "                    h = int( h * self.len_chack)\n",
        "                    k = int( k * self.len_chack)\n",
        "                    images[:, k:h] = 0\n",
        "\n",
        "            if random.random()<0.5:\n",
        "                    idy = random.randint(0, len(self.background_audio) - 1)\n",
        "                    mel = self.background_audio[idy]\n",
        "                    mel = mel[np.random.choice(len(mel))]\n",
        "                    mel = np.concatenate((np.zeros((self.n_mels, train_len_chack)), mel), axis=1)\n",
        "                    mel = np.concatenate((mel, np.zeros((self.n_mels, train_len_chack))), axis=1)\n",
        "                    start = random.randint(0, mel.shape[1] - train_len_chack - 1)\n",
        "                    mel = mel[:, start : start + train_len_chack]\n",
        "                    mel = random_power(mel)\n",
        "                    # mel = librosa.power_to_db(mel.astype(np.float32), ref=np.max)\n",
        "                    # mel = (mel+80)/80\n",
        "                    images = (\n",
        "                        images\n",
        "                        + mel\n",
        "                        / (mel.max() + 0.0000001)\n",
        "                        * (random.random() * 1 + 0.5)\n",
        "                        * images.max()\n",
        "                    )\n",
        "        images = images.astype(\"float32\", copy=False)\n",
        "        images = librosa.power_to_db(images, ref=np.max)\n",
        "        images = (images + 80) / 80\n",
        "        images = random_power(images, power=2, c=0.7)\n",
        "        images = np.nan_to_num(images)\n",
        "        if self.is_train:\n",
        "            images = mono_to_color_train_v2(images, self.len_chack)\n",
        "        else:\n",
        "            images = mono_to_color_v2(images)\n",
        "        # if random.random() < 0.3:\n",
        "        #     images = time_shift_spectrogram(images)\n",
        "        #     images = images.transpose(2, 0, 1)\n",
        "\n",
        "        for second_label in secondary_label:\n",
        "            if second_label in CFG.target_columns:\n",
        "                t[CFG.target_columns.index(second_label)] = 0.3\n",
        "        return images, t\n",
        "\n",
        "def one_step( xb,  yb, net,  criterion, optimizer, scheduler=None, mixup_proba=0.5, alpha=5, label_smoothing=True):\n",
        "    xb, yb = xb.to(CFG.DEVICE),yb.to(CFG.DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    if np.random.rand() < mixup_proba:\n",
        "            xb, y_a, y_b, _ = mixup_data(xb.cuda(), yb.cuda(), alpha=alpha)\n",
        "            yb = torch.clamp(y_a + y_b, 0, 1)\n",
        "    # if label_smoothing:\n",
        "    #     yb = smooth_label(yb)\n",
        "    o = net(xb)\n",
        "    loss = criterion(o, yb)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    o = o[\"logit\"]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        l = loss.item()\n",
        "        o = o.sigmoid()\n",
        "        yb = (yb > 0.5)*1.0\n",
        "        lrap = label_ranking_average_precision_score(yb.cpu().numpy(), o.cpu().numpy())\n",
        "        o = (o > 0.5)*1.0\n",
        "        prec = (o*yb).sum()/(1e-6 + o.sum())\n",
        "        rec = (o*yb).sum()/(1e-6 + yb.sum())\n",
        "        f1 = 2*prec*rec/(1e-6+prec+rec)\n",
        "    return l, lrap, f1.item(), rec.item(), prec.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(net, criterion, val_laoder):\n",
        "    net.eval()\n",
        "\n",
        "    os, y = [], []\n",
        "    val_laoder = tqdm_notebook(val_laoder, leave = False, total=len(val_laoder))\n",
        "\n",
        "    for icount, (xb, yb) in  enumerate(val_laoder):\n",
        "        y.append(yb.to(CFG.DEVICE))\n",
        "        xb = xb.to(CFG.DEVICE)\n",
        "        o = net(xb)[\"logit\"]\n",
        "        os.append(o)\n",
        "    y = torch.cat(y)\n",
        "    o = torch.cat(os)\n",
        "    l = nn.BCEWithLogitsLoss()(o, y).item()\n",
        "    o = o.sigmoid()\n",
        "    y = (y > 0.5)*1.0\n",
        "    lrap = label_ranking_average_precision_score(y.cpu().numpy(), o.cpu().numpy())\n",
        "    o = (o > 0.5)*1.0\n",
        "    prec = ((o*y).sum()/(1e-6 + o.sum())).item()\n",
        "    rec = ((o*y).sum()/(1e-6 + y.sum())).item()\n",
        "    f1 = 2*prec*rec/(1e-6+prec+rec)\n",
        "    return l, lrap, f1, rec, prec,\n",
        "\n",
        "def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder, n=10):\n",
        "  net.train()\n",
        "  l, lrap, prec, rec, f1, icount = 0.,0.,0.,0., 0., 0\n",
        "  train_laoder = tqdm_notebook(train_laoder, leave = False)\n",
        "  epoch_bar = train_laoder\n",
        "  cnt = n \n",
        "  for (xb, yb) in  epoch_bar:\n",
        "      # epoch_bar.set_description(\"----|----|----|----|---->\")\n",
        "      cnt -= 1\n",
        "      _l, _lrap, _f1, _rec, _prec = one_step(xb, yb, net, criterion, optimizer)\n",
        "      l += _l\n",
        "      lrap += _lrap\n",
        "      f1 += _f1\n",
        "      rec += _rec\n",
        "      prec += _prec\n",
        "\n",
        "      icount += 1\n",
        "        \n",
        "      if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n",
        "          epoch_bar.set_postfix(\n",
        "            loss=\"{:.6f}\".format(l/icount),\n",
        "            lrap=\"{:.3f}\".format(lrap/icount),\n",
        "            prec=\"{:.3f}\".format(prec/icount),\n",
        "            rec=\"{:.3f}\".format(rec/icount),\n",
        "            f1=\"{:.3f}\".format(f1/icount),\n",
        "          )\n",
        "  l /= icount\n",
        "  lrap /= icount\n",
        "  f1 /= icount\n",
        "  rec /= icount\n",
        "  prec /= icount\n",
        "  \n",
        "  l_val, lrap_val, f1_val, rec_val, prec_val = evaluate(net, criterion, val_laoder)\n",
        "\n",
        "  scheduler.step()  \n",
        "  return (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val)\n",
        "\n",
        "def one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None,background_audio=None):\n",
        "  save_root = Path(save_root) or CFG.MODEL_ROOT\n",
        "  saver = AutoSave(root=save_root, name=f\"birdclef_{model_name}_fold{fold}\", metric=\"f1_val\")\n",
        "  config_model =   {model_name: \"efficientnet-b1\", \"pretrained\": True, \"num_classes\": 397}\n",
        "\n",
        "  # net =  EfficientNetSED(\"efficientnet-b1\", True, 397).to(DEVICE)\n",
        "  net =  TimmSED(model_name, True, 397).to(CFG.DEVICE)\n",
        "  \n",
        "  #resnext_meta().to(DEVICE)\n",
        "  criterion = ImprovedPANNsLoss(weights=[1.0 , 0.5])\n",
        "  optimizer = optim.AdamW(net.parameters(), lr=CFG.lr)\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,  T_max=epochs)\n",
        "  train_data = BirdClefDataset( meta=train_set, sr=CFG.sr, duration=CFG.duration,background_audio=background_audio, is_train=True)\n",
        "  train_laoder = DataLoader(train_data, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=True, pin_memory=True)\n",
        "  val_data = BirdClefDataset( meta=val_set,  sr=CFG.sr, duration=CFG.duration, is_train=False)\n",
        "  val_laoder = DataLoader(val_data, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=False, pin_memory=True)\n",
        "  epochs_bar = tqdm(list(range(epochs)), leave=False)\n",
        "  for epoch  in epochs_bar:\n",
        "    epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n",
        "    net.train()\n",
        "    (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val) = one_epoch(\n",
        "        net=net,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_laoder=train_laoder,\n",
        "        val_laoder=val_laoder,\n",
        "      )\n",
        "    epochs_bar.set_postfix(\n",
        "    loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n",
        "    prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n",
        "    rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n",
        "    f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n",
        "    lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n",
        "    )\n",
        "    print(\n",
        "        \"[{epoch:02d}] loss: {loss} lrap: {lrap} f1: {f1} rec: {rec} prec: {prec}\".format(\n",
        "            epoch=epoch,\n",
        "            loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n",
        "            prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n",
        "            rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n",
        "            f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n",
        "            lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n",
        "        )\n",
        "    )\n",
        "    if save:\n",
        "      metrics = {\n",
        "          \"loss\": l, \"lrap\": lrap, \"f1\": f1, \"rec\": rec, \"prec\": prec,\n",
        "          \"loss_val\": l_val, \"lrap_val\": lrap_val, \"f1_val\": f1_val, \"rec_val\": rec_val, \"prec_val\": prec_val,\n",
        "          \"epoch\": epoch,\n",
        "      }\n",
        "      saver.log(net, metrics)\n",
        "  torch.save(net.state_dict(), save_root/f\"last_epochs_fold{fold}.pth\")\n",
        "\n",
        "def train(df,model_name, epochs=20, save=True, n_splits=5, seed=177, save_root=None,background_audio=None, suffix=\"\"):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # environment\n",
        "    set_seed(CFG.seed)\n",
        "    device = get_device()\n",
        "    # validation\n",
        "    # data\n",
        "    save_root.mkdir(exist_ok=True, parents=True)\n",
        "    for i in range(1,5):\n",
        "        if i not in CFG.folds:\n",
        "            continue\n",
        "        save_root = save_root/f\"fold-{i}\"\n",
        "        save_root.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        print(\"=\" * 120)\n",
        "        print(f\"Fold {i} Training\")\n",
        "        print(\"=\" * 120)\n",
        "        trn_df = df[df['fold']!=i].reset_index(drop=True)\n",
        "        val_df = df[df['fold']==i].reset_index(drop=True)\n",
        "        one_fold(model_name, fold=i, train_set=trn_df , val_set=val_df , epochs=CFG.epochs,background_audio=background_audio, save=save, save_root=save_root)\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "MEL_PATHS = \"/content/gdrive/MyDrive/Kaggle/kkiller-dataset/rich_train_metadata.csv\"\n",
        "audio_path = Path(\"/content/audio_images\") \n",
        "background_audio = load_data(\"/content/audio_cache/*\")\n",
        "print(len(background_audio))\n",
        "MODEL_NAMES = [\"mobilenetv3_rw\"]\n",
        "df = pd.read_csv(MEL_PATHS)\n",
        "df[\"impath\"] = df.apply(lambda row: audio_path/\"{}/{}.npy\".format(row.primary_label, row.filename), axis=1) \n",
        "for model_name in MODEL_NAMES:\n",
        "    MODEL_ROOT = Path(f\"/content/gdrive/MyDrive/Kaggle/{model_name}_SED_MIX3AUDIO\")\n",
        "    print(\"\\n\\n###########################################\", model_name.upper())\n",
        "    try:\n",
        "        train(df, model_name, epochs=35, save_root=MODEL_ROOT,background_audio=background_audio, suffix=f\"_sr{32000}_d{20}_v1_v1\")\n",
        "    except Exception as e:\n",
        "        raise ValueError() from  e\n"
      ],
      "id": "6KfQdZN_kXHI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 32/32 [00:00<00:00, 836.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/birdcall-2021\n",
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "########################################### MOBILENETV3_RW\n",
            "========================================================================================================================\n",
            "Fold 1 Training\n",
            "========================================================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_100-35495452.pth\" to /root/.cache/torch/hub/checkpoints/mobilenetv3_100-35495452.pth\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--> [EPOCH 00]:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc23ca52e64940ecb3b8b57e7af9116f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1258.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b629a00d612844449df2ba355fea8052",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=315.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "certified-garbage"
      },
      "source": [
        "## Libraries"
      ],
      "id": "certified-garbage"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "banned-continuity"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/birdcall-2021\")\n",
        "from src.configuration import *\n",
        "from src.criterion import *\n",
        "from src.models import *\n",
        "from src.utils import *\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import timm\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as torchdata\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import librosa as lb\n",
        "import librosa.display as lbd\n",
        "import soundfile as sf\n",
        "from  soundfile import SoundFile\n",
        "import pandas as pd\n",
        "from  IPython.display import Audio\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from  torch.utils.data import Dataset, DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "import os, random, gc\n",
        "import re, time, json\n",
        "from  ast import literal_eval\n",
        "from IPython.display import Audio\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from importlib import reload  \n",
        "import re\n",
        "from torchvision import transforms\n"
      ],
      "id": "banned-continuity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "celtic-representative"
      },
      "source": [
        "## Config"
      ],
      "id": "celtic-representative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "several-seattle"
      },
      "source": [
        "class CFG:\n",
        "    ######################\n",
        "    # Globals #\n",
        "    ######################\n",
        "    seed = 2021\n",
        "    epochs = 50\n",
        "    train = True\n",
        "    folds = [0,1,2,3,4]\n",
        "    batch_size = 60\n",
        "    weight_decay = 1e-8\n",
        "    lr = 1e-3\n",
        "    num_workers = 4\n",
        "    num_classes = 397\n",
        "    sr = 32000\n",
        "    sample_rate = 32000\n",
        "    duration = 7\n",
        "    nmels = 224\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    target_columns = [\n",
        "        'acafly', 'acowoo', 'aldfly', 'ameavo', 'amecro',\n",
        "        'amegfi', 'amekes', 'amepip', 'amered', 'amerob',\n",
        "        'amewig', 'amtspa', 'andsol1', 'annhum', 'astfly',\n",
        "        'azaspi1', 'babwar', 'baleag', 'balori', 'banana',\n",
        "        'banswa', 'banwre1', 'barant1', 'barswa', 'batpig1',\n",
        "        'bawswa1', 'bawwar', 'baywre1', 'bbwduc', 'bcnher',\n",
        "        'belkin1', 'belvir', 'bewwre', 'bkbmag1', 'bkbplo',\n",
        "        'bkbwar', 'bkcchi', 'bkhgro', 'bkmtou1', 'bknsti', 'blbgra1',\n",
        "        'blbthr1', 'blcjay1', 'blctan1', 'blhpar1', 'blkpho',\n",
        "        'blsspa1', 'blugrb1', 'blujay', 'bncfly', 'bnhcow', 'bobfly1',\n",
        "        'bongul', 'botgra', 'brbmot1', 'brbsol1', 'brcvir1', 'brebla',\n",
        "        'brncre', 'brnjay', 'brnthr', 'brratt1', 'brwhaw', 'brwpar1',\n",
        "        'btbwar', 'btnwar', 'btywar', 'bucmot2', 'buggna', 'bugtan',\n",
        "        'buhvir', 'bulori', 'burwar1', 'bushti', 'butsal1', 'buwtea',\n",
        "        'cacgoo1', 'cacwre', 'calqua', 'caltow', 'cangoo', 'canwar',\n",
        "        'carchi', 'carwre', 'casfin', 'caskin', 'caster1', 'casvir',\n",
        "        'categr', 'ccbfin', 'cedwax', 'chbant1', 'chbchi', 'chbwre1',\n",
        "        'chcant2', 'chispa', 'chswar', 'cinfly2', 'clanut', 'clcrob',\n",
        "        'cliswa', 'cobtan1', 'cocwoo1', 'cogdov', 'colcha1', 'coltro1',\n",
        "        'comgol', 'comgra', 'comloo', 'commer', 'compau', 'compot1',\n",
        "        'comrav', 'comyel', 'coohaw', 'cotfly1', 'cowscj1', 'cregua1',\n",
        "        'creoro1', 'crfpar', 'cubthr', 'daejun', 'dowwoo', 'ducfly', 'dusfly',\n",
        "        'easblu', 'easkin', 'easmea', 'easpho', 'eastow', 'eawpew', 'eletro',\n",
        "        'eucdov', 'eursta', 'fepowl', 'fiespa', 'flrtan1', 'foxspa', 'gadwal',\n",
        "        'gamqua', 'gartro1', 'gbbgul', 'gbwwre1', 'gcrwar', 'gilwoo',\n",
        "        'gnttow', 'gnwtea', 'gocfly1', 'gockin', 'gocspa', 'goftyr1',\n",
        "        'gohque1', 'goowoo1', 'grasal1', 'grbani', 'grbher3', 'grcfly',\n",
        "        'greegr', 'grekis', 'grepew', 'grethr1', 'gretin1', 'greyel',\n",
        "        'grhcha1', 'grhowl', 'grnher', 'grnjay', 'grtgra', 'grycat',\n",
        "        'gryhaw2', 'gwfgoo', 'haiwoo', 'heptan', 'hergul', 'herthr',\n",
        "        'herwar', 'higmot1', 'hofwoo1', 'houfin', 'houspa', 'houwre',\n",
        "        'hutvir', 'incdov', 'indbun', 'kebtou1', 'killde', 'labwoo', 'larspa',\n",
        "        'laufal1', 'laugul', 'lazbun', 'leafly', 'leasan', 'lesgol', 'lesgre1',\n",
        "        'lesvio1', 'linspa', 'linwoo1', 'littin1', 'lobdow', 'lobgna5', 'logshr',\n",
        "        'lotduc', 'lotman1', 'lucwar', 'macwar', 'magwar', 'mallar3', 'marwre',\n",
        "        'mastro1', 'meapar', 'melbla1', 'monoro1', 'mouchi', 'moudov', 'mouela1',\n",
        "        'mouqua', 'mouwar', 'mutswa', 'naswar', 'norcar', 'norfli', 'normoc', 'norpar',\n",
        "        'norsho', 'norwat', 'nrwswa', 'nutwoo', 'oaktit', 'obnthr1', 'ocbfly1',\n",
        "        'oliwoo1', 'olsfly', 'orbeup1', 'orbspa1', 'orcpar', 'orcwar', 'orfpar',\n",
        "        'osprey', 'ovenbi1', 'pabspi1', 'paltan1', 'palwar', 'pasfly', 'pavpig2',\n",
        "        'phivir', 'pibgre', 'pilwoo', 'pinsis', 'pirfly1', 'plawre1', 'plaxen1',\n",
        "        'plsvir', 'plupig2', 'prowar', 'purfin', 'purgal2', 'putfru1', 'pygnut',\n",
        "        'rawwre1', 'rcatan1', 'rebnut', 'rebsap', 'rebwoo', 'redcro', 'reevir1',\n",
        "        'rehbar1', 'relpar', 'reshaw', 'rethaw', 'rewbla', 'ribgul', 'rinkin1',\n",
        "        'roahaw', 'robgro', 'rocpig', 'rotbec', 'royter1', 'rthhum', 'rtlhum',\n",
        "        'ruboro1', 'rubpep1', 'rubrob', 'rubwre1', 'ruckin', 'rucspa1', 'rucwar',\n",
        "        'rucwar1', 'rudpig', 'rudtur', 'rufhum', 'rugdov', 'rumfly1', 'runwre1',\n",
        "        'rutjac1', 'saffin', 'sancra', 'sander', 'savspa', 'saypho', 'scamac1',\n",
        "        'scatan', 'scbwre1', 'scptyr1', 'scrtan1', 'semplo', 'shicow', 'sibtan2',\n",
        "        'sinwre1', 'sltred', 'smbani', 'snogoo', 'sobtyr1', 'socfly1', 'solsan',\n",
        "        'sonspa', 'soulap1', 'sposan', 'spotow', 'spvear1', 'squcuc1', 'stbori',\n",
        "        'stejay', 'sthant1', 'sthwoo1', 'strcuc1', 'strfly1', 'strsal1', 'stvhum2',\n",
        "        'subfly', 'sumtan', 'swaspa', 'swathr', 'tenwar', 'thbeup1', 'thbkin',\n",
        "        'thswar1', 'towsol', 'treswa', 'trogna1', 'trokin', 'tromoc', 'tropar',\n",
        "        'tropew1', 'tuftit', 'tunswa', 'veery', 'verdin', 'vigswa', 'warvir',\n",
        "        'wbwwre1', 'webwoo1', 'wegspa1', 'wesant1', 'wesblu', 'weskin', 'wesmea',\n",
        "        'westan', 'wewpew', 'whbman1', 'whbnut', 'whcpar', 'whcsee1', 'whcspa',\n",
        "        'whevir', 'whfpar1', 'whimbr', 'whiwre1', 'whtdov', 'whtspa', 'whwbec1',\n",
        "        'whwdov', 'wilfly', 'willet1', 'wilsni1', 'wiltur', 'wlswar', 'wooduc',\n",
        "        'woothr', 'wrenti', 'y00475', 'yebcha', 'yebela1', 'yebfly', 'yebori1',\n",
        "        'yebsap', 'yebsee1', 'yefgra1', 'yegvir', 'yehbla', 'yehcar1', 'yelgro',\n",
        "        'yelwar', 'yeofly1', 'yerwar', 'yeteup1', 'yetvir']\n",
        "\n",
        "MODEL_NAMES = [\n",
        "               \"RESNET50_SED\"\n",
        "              #  \"vit_base_patch16_224\"\n",
        "      # \"Perceiver\",\n",
        "] \n",
        "MODEL_ROOT = Path(\"/content/gdrive/MyDrive/Kaggle/kkiller-dataset/RESNET50_SED_MIX3AUDIO\")\n",
        "MEL_PATHS = \"/content/gdrive/MyDrive/Kaggle/kkiller-dataset/rich_train_metadata.csv\"\n",
        "audio_path = Path(\"/content/audio_images\")\n",
        "SOUDSCAPE_PATH = \"/content/soundscape/\"\n",
        "SR = 32_000\n",
        "NUM_CLASSES= 397\n",
        "DURATION = 20\n",
        "DEVICE= \"cuda:0\"\n",
        "\n",
        "\n",
        "EFFNETB6_EMB_DIM = 2304\n",
        "EFFNETB5_EMB_DIM = 2048\n",
        "EFFNETB4_EMB_DIM = 1792\n",
        "EFFNETB3_EMB_DIM = 1536\n",
        "EFFNETB1_EMB_DIM = 1280\n",
        "RESNEST50_FAST_EMB_DIM = 2048\n",
        "RESNEXT50_EMB_DIM = 2048\n",
        "RESNEXT101_EMB_DIM = 2048"
      ],
      "id": "several-seattle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intended-dealing"
      },
      "source": [
        "## Utilities"
      ],
      "id": "intended-dealing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMDAtZH0Rrrp"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "class SimpleBalanceClassSampler(Sampler):\n",
        "\n",
        "    def __init__(self, targets, classes_num):\n",
        "\n",
        "        self.targets = targets\n",
        "        self.classes_num = classes_num\n",
        "        self.max_num=100 #hardcode \n",
        "        \n",
        "        self.indexes_per_class = []\n",
        "        for k in range(self.classes_num):\n",
        "            self.indexes_per_class.append(\n",
        "                np.where(self.targets[:, k] == 1)[0])\n",
        "        \n",
        "        self.length = self.classes_num * self.max_num\n",
        "\n",
        "    def __iter__(self):\n",
        "        \n",
        "        all_indexs = []\n",
        "        \n",
        "        for k in range(self.classes_num):\n",
        "            if len(self.indexes_per_class[k]) == self.max_num:\n",
        "                all_indexs.append(self.indexes_per_class[k])\n",
        "            elif len(self.indexes_per_class[k]) > self.max_num:\n",
        "                random_choice = np.random.choice(self.indexes_per_class[k], int(self.max_num), replace=True)\n",
        "                all_indexs.append(np.array(list(random_choice)))\n",
        "            else:\n",
        "                gap = self.max_num - len(self.indexes_per_class[k])\n",
        "                random_choice = np.random.choice(self.indexes_per_class[k], int(gap), replace=True)\n",
        "                all_indexs.append(np.array(list(random_choice) + list(self.indexes_per_class[k])))\n",
        "                \n",
        "        l = np.stack(all_indexs).T\n",
        "        l = l.reshape(-1)\n",
        "        random.shuffle(l)\n",
        "        return iter(l)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.length)\n",
        "        \n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def mixup_data(x, y, alpha=5):\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
        "    index = torch.randperm(x.size()[0]).cuda()\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def smooth_label(y , alpha=0.01):\n",
        "    y = y * (1 - alpha)\n",
        "    y[y == 0] = alpha\n",
        "    return y\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def init_logger(log_file='train.log'):\n",
        "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return loggerutil\n",
        "\n",
        "class AutoSave:\n",
        "    def __init__(self, top_k=2, metric=\"f1\", mode=\"min\", root=None, name=\"ckpt\"):\n",
        "        self.top_k = top_k\n",
        "        self.logs = []\n",
        "        self.metric = metric\n",
        "        self.mode = mode\n",
        "        self.root = Path(root)\n",
        "        assert self.root.exists()\n",
        "        self.name = name\n",
        "\n",
        "        self.top_models = []\n",
        "        self.top_metrics = []\n",
        "\n",
        "    def log(self, model, metrics):\n",
        "        metric = metrics[self.metric]\n",
        "        rank = self.rank(metric)\n",
        "\n",
        "        self.top_metrics.insert(rank+1, metric)\n",
        "        if len(self.top_metrics) > self.top_k:\n",
        "            self.top_metrics.pop(0)\n",
        "\n",
        "        self.logs.append(metrics)\n",
        "        self.save(model, metric, rank, metrics[\"epoch\"])\n",
        "\n",
        "\n",
        "    def save(self, model, metric, rank, epoch):\n",
        "        t = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "        name = \"{}_epoch_{:02d}_{}_{:.04f}_{}\".format(self.name, epoch, self.metric, metric, t)\n",
        "        name = re.sub(r\"[^\\w_-]\", \"\", name) + \".pth\"\n",
        "        path = self.root.joinpath(name)\n",
        "\n",
        "        old_model = None\n",
        "        self.top_models.insert(rank+1, name)\n",
        "        if len(self.top_models) > self.top_k:\n",
        "            old_model = self.root.joinpath(self.top_models[0])\n",
        "            self.top_models.pop(0)      \n",
        "\n",
        "        torch.save(model.state_dict(), path.as_posix())\n",
        "\n",
        "        if old_model is not None:\n",
        "            old_model.unlink()\n",
        "\n",
        "        self.to_json()\n",
        "\n",
        "\n",
        "    def rank(self, val):\n",
        "        r = -1\n",
        "        for top_val in self.top_metrics:\n",
        "            if val <= top_val:\n",
        "                return r\n",
        "            r += 1\n",
        "\n",
        "        return r\n",
        "  \n",
        "    def to_json(self):\n",
        "    # t = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "        name = \"{}_logs\".format(self.name)\n",
        "        name = re.sub(r\"[^\\w_-]\", \"\", name) + \".json\"\n",
        "        path = self.root.joinpath(name)\n",
        "\n",
        "        with path.open(\"w\") as f:\n",
        "            json.dump(self.logs, f, indent=2)\n"
      ],
      "id": "VMDAtZH0Rrrp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eligible-detail"
      },
      "source": [
        "## Dataset and Data Augmentations\n",
        "\n",
        "In this section, I define dataset that crops 20 second chunk. The output of this dataset is a pair of waveform and corresponding label."
      ],
      "id": "eligible-detail"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcEudQ2OKcHj"
      },
      "source": [
        "def get_df(mel_paths=MEL_PATHS):\n",
        "  df = pd.read_csv(MEL_PATHS)\n",
        "  df[\"secondary_labels\"] = df[\"secondary_labels\"].apply(literal_eval)\n",
        "  return  df\n",
        "\n",
        "def load_soundscape(df):\n",
        "    def load_row(row):\n",
        "        return row.row_id, np.load(SOUDSCAPE_PATH + str(row.row_id)+\".npy\")\n",
        "    pool = joblib.Parallel(4)\n",
        "    mapper = joblib.delayed(load_row)\n",
        "    tasks = [mapper(row) for row in df.itertuples(False)]\n",
        "    res = pool(tqdm(tasks))\n",
        "    res = dict(res)\n",
        "    return res\n",
        "\n",
        "def load_data(df):\n",
        "    def load_row(row):\n",
        "        return row.filename, np.load(str(row.impath))[:15]\n",
        "    pool = joblib.Parallel(4)\n",
        "    mapper = joblib.delayed(load_row)\n",
        "    tasks = [mapper(row) for row in df.itertuples(False)]\n",
        "    res = pool(tqdm(tasks))\n",
        "    res = dict(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "def random_power(images, power = 1.5, c= 0.7):\n",
        "    images = images - images.min()\n",
        "    images = images/(images.max()+0.0000001)\n",
        "    images = images**(random.random()*power + c)\n",
        "    return images\n",
        "def mono_to_color(X: np.ndarray, mean=0.5, std=0.5, eps=1e-6):\n",
        "    trans = transforms.Compose([transforms.ToPILImage(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    X = np.stack([X, X, X], axis=-1)\n",
        "    V = (255 * X).astype(np.uint8)\n",
        "    V = (trans(V)+1)/2\n",
        "    return V"
      ],
      "id": "KcEudQ2OKcHj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX55_RxbK7IE"
      },
      "source": [
        "# We cache the train set to reduce training time\n",
        "# soundscape = pd.read_csv(SOUDSCAPE_PATH + \"train_soundscape_labels.csv\")\n",
        "# soundscape_image_store = load_soundscape(soundscape)\n",
        "df = pd.read_csv(MEL_PATHS)\n",
        "df[\"impath\"] = df.apply(lambda row: audio_path/\"{}/{}.npy\".format(row.primary_label, row.filename), axis=1) \n",
        "df.head()\n",
        "# audio_image_store = load_data(df)"
      ],
      "id": "GX55_RxbK7IE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzURWoViSXjb"
      },
      "source": [
        "\n",
        "class BirdClefDataset(Dataset):\n",
        "    def __init__(self,  meta, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n",
        "        self.meta = meta.copy().reset_index(drop=True)\n",
        "        self.sr = sr\n",
        "        self.is_train = is_train\n",
        "        self.num_classes = num_classes\n",
        "        self.duration = duration\n",
        "        self.audio_length = self.duration*self.sr\n",
        "        self.n_mels = 128\n",
        "        self.len_chack = 626\n",
        "        self.stop_border = 0.3 # Probability of stopping mixing | Вероятность прервать смешивание\n",
        "        self.level_noise = 0.05 # level noise | Уровень шума\n",
        "        self.div_coef = 100 # signal amplification during mixing | Усиления сигнала при смешивании\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.meta.iloc[idx]\n",
        "        # imagesx = np.load(row.impath)\n",
        "        ebird_code = row[\"primary_label\"]\n",
        "        secondary_label = row[\"secondary_labels\"]\n",
        "        meta_data = np.array(row[[\"latitude\", \"longitude\"]])\n",
        "        data = np.array(list(map(int, row[\"date\"].split(\"-\"))))\n",
        "        meta_data = np.hstack((meta_data, data)).astype(np.float32)\n",
        "        melspecs = np.load(row['impath'], allow_pickle=True)\n",
        "        t_pobs = melspecs.item().get('probs')\n",
        "        try:\n",
        "            images = melspecs.item().get('images')[np.random.choice(len(t_pobs), size=1, p=t_pobs)[0]]\n",
        "        except:\n",
        "            images = melspecs.item().get('images')[np.random.choice(len(t_pobs))]\n",
        "        t = np.zeros(self.num_classes, dtype=np.float32)  # Label smoothing\n",
        "        # images = (images+80)/80\n",
        "        # Add noise | Добавить шум\n",
        "        # Add white noise | Добавить белый шум \n",
        "        t[CFG.target_columns.index(ebird_code)] = 1.0\n",
        "        if self.is_train:   \n",
        "             \n",
        "            if random.random()<0.9:\n",
        "                    images = images + (np.random.sample((self.n_mels,self.len_chack)).astype(np.float32)+9) * images.mean() * self.level_noise * (np.random.sample() + 0.3)\n",
        "                \n",
        "                # Add pink noise | Добавить розовый шум\n",
        "            if random.random()<0.9:\n",
        "                    r = random.randint(1,self.n_mels)\n",
        "                    pink_noise = np.array([np.concatenate((1 - np.arange(r)/r,np.zeros(self.n_mels-r)))]).T\n",
        "                    images = images + (np.random.sample((self.n_mels,self.len_chack)).astype(np.float32)+9) * 2  * images.mean() * self.level_noise * (np.random.sample() + 0.3)\n",
        "                \n",
        "                # Add bandpass noise | Добавить полосовой шум\n",
        "            if random.random()<0.9:\n",
        "                    a = random.randint(0, self.n_mels//2)\n",
        "                    b = random.randint(a+20, self.n_mels)\n",
        "                    images[a:b,:] = images[a:b,:] + (np.random.sample((b-a,self.len_chack)).astype(np.float32)+9) * 0.05 * images.mean() * self.level_noise  * (np.random.sample() + 0.3)\n",
        "                \n",
        "                \n",
        "                # Lower the upper frequencies | Понизить верхние частоты\n",
        "            if random.random()<0.5:\n",
        "                    images = images - images.min()\n",
        "                    r = random.randint(self.n_mels//2,self.n_mels)\n",
        "                    x = random.random()/2\n",
        "                    pink_noise = np.array([np.concatenate((1-np.arange(r)*x/r,np.zeros(self.n_mels-r)-x+1))]).T\n",
        "                    images = images*pink_noise\n",
        "                    images = images/(images.max()+0.0000001)\n",
        "\n",
        "            if random.random()<0.1:\n",
        "                    w = np.random.uniform(0.2, 0.5)\n",
        "                    images = (images + w*imagesx[np.random.choice(len(imagesx))])/(1+w)\n",
        "\n",
        "            if random.random()<0.5:\n",
        "                    k = np.random.uniform(0.0, 0.7)\n",
        "                    h = np.random.uniform(k, k+0.3)\n",
        "                    h = int( h * self.len_chack)\n",
        "                    k = int( k * self.len_chack)\n",
        "                    images[:, k:h] = 0\n",
        "\n",
        "        # Change the contrast | Изменить контрастность\n",
        "        images = images.astype(\"float32\", copy=False)\n",
        "        \n",
        "        images = librosa.power_to_db(images, ref=np.max)\n",
        "        images = (images+80)/80\n",
        "        images = random_power(images, power = 2, c= 0.7)\n",
        "        images = np.nan_to_num(images)\n",
        "        images = mono_to_color(images)\n",
        "        for second_label in secondary_label:\n",
        "            if second_label in CFG.target_columns:\n",
        "                t[CFG.target_columns.index(second_label)] = 0.3\n",
        "        return images, meta_data, t\n",
        "\n",
        "\n",
        "class BirdClefDataset_V2(Dataset):\n",
        "    def __init__(self,  meta, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n",
        "        self.meta = meta.copy().reset_index(drop=True)\n",
        "        self.sr = sr\n",
        "        self.is_train = is_train\n",
        "        self.num_classes = num_classes\n",
        "        self.duration = duration\n",
        "        self.audio_length = self.duration*self.sr\n",
        "        self.n_mels = 128\n",
        "        self.len_chack = 626\n",
        "        self.stop_border = 0.8 # Probability of stopping mixing | Вероятность прервать смешивание\n",
        "        self.level_noise = 0.05 # level noise | Уровень шума\n",
        "        self.div_coef = 100 # signal amplification during mixing | Усиления сигнала при смешивании\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        idx2 = random.randint(0, len(self.meta)-1) # Second file | Второй файл\n",
        "        idx3 = random.randint(0, len(self.meta)-1) # Third file | Третий файл\n",
        "        y = np.zeros(self.num_classes, dtype=np.float32)\n",
        "        birds, background = [],[]\n",
        "        images = np.zeros((self.n_mels, self.len_chack)).astype(np.float32)            \n",
        "        for i,idy in enumerate([idx,idx2,idx3]):\n",
        "            # Choosing a record with a bird | Выбираем запись с птицей\n",
        "            row = self.meta.iloc[idy]\n",
        "            # imagesx = np.load(row.impath)\n",
        "            ebird_code = row[\"primary_label\"]\n",
        "            secondary_label = row[\"secondary_labels\"]\n",
        "            melspecs = np.load(row['impath'], allow_pickle=True)\n",
        "            t_pobs = melspecs.item().get('probs')\n",
        "            try:\n",
        "                melspecs = melspecs.item().get('images')[np.random.choice(len(t_pobs), size=1, p=t_pobs)[0]]\n",
        "            except:\n",
        "                melspecs = melspecs.item().get('images')[np.random.choice(len(t_pobs))]\n",
        "\n",
        "            # Birds in the file | Птицы в файле\n",
        "            birds.append(CFG.target_columns.index(ebird_code))\n",
        "            # Birds in the background | Птицы на фоне     \n",
        "            for second_label in secondary_label:\n",
        "                if second_label in CFG.target_columns:\n",
        "                        background.append(CFG.target_columns.index(second_label))\n",
        "           \n",
        "            # Change the contrast | Изменить контрастность\n",
        "            melspecs = random_power(melspecs, power = 3, c= 0.5)\n",
        "            images = images + melspecs*(random.random() * self.div_coef + 1)            \n",
        "            if random.random()<self.stop_border:\n",
        "                break\n",
        "            if self.is_train:\n",
        "                break\n",
        "\n",
        "        images = librosa.power_to_db(images.astype(np.float32), ref=np.max)\n",
        "        images = (images+80)/80\n",
        "        if self.is_train:          \n",
        "            if random.random()<0.9:\n",
        "                images = images + (np.random.sample((self.n_mels,self.len_chack)).astype(np.float32)+9) * images.mean() * self.level_noise * (np.random.sample() + 0.3)\n",
        "            \n",
        "            # Add pink noise | Добавить розовый шум\n",
        "            if random.random()<0.9:\n",
        "                r = random.randint(1,self.n_mels)\n",
        "                pink_noise = np.array([np.concatenate((1 - np.arange(r)/r,np.zeros(self.n_mels-r)))]).T\n",
        "                images = images + (np.random.sample((self.n_mels,self.len_chack)).astype(np.float32)+9) * 2  * images.mean() * self.level_noise * (np.random.sample() + 0.3)\n",
        "            \n",
        "            # Add bandpass noise | Добавить полосовой шум\n",
        "            if random.random()<0.9:\n",
        "                a = random.randint(0, self.n_mels//2)\n",
        "                b = random.randint(a+20, self.n_mels)\n",
        "                images[a:b,:] = images[a:b,:] + (np.random.sample((b-a,self.len_chack)).astype(np.float32)+9) * 0.05 * images.mean() * self.level_noise  * (np.random.sample() + 0.3)\n",
        "            \n",
        "            \n",
        "            # Lower the upper frequencies | Понизить верхние частоты\n",
        "            if random.random()<0.5:\n",
        "                images = images - images.min()\n",
        "                r = random.randint(self.n_mels//2,self.n_mels)\n",
        "                x = random.random()/2\n",
        "                pink_noise = np.array([np.concatenate((1-np.arange(r)*x/r,np.zeros(self.n_mels-r)-x+1))]).T\n",
        "                images = images*pink_noise\n",
        "                images = images/images.max()\n",
        "        \n",
        "        # Change the contrast | Изменить контрастность\n",
        "        images = random_power(images, power = 2, c= 0.7)\n",
        "        images = mono_to_color(images)\n",
        "        for bird in background:\n",
        "            if bird < len(y):\n",
        "                y[bird]=0.3\n",
        "        for bird in birds:\n",
        "            #if not bird==264:\n",
        "            y[bird]=1\n",
        "        return images, y\n"
      ],
      "id": "NzURWoViSXjb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stKiObHdSa_y"
      },
      "source": [
        "ds = BirdClefDataset_V2(df,  SR, True)"
      ],
      "id": "stKiObHdSa_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTOpCC4KTxsS"
      },
      "source": [
        "plt.imshow(ds[0][0].transpose(0,2))"
      ],
      "id": "XTOpCC4KTxsS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGMnEd2vP-TT"
      },
      "source": [
        "ds[0][0]"
      ],
      "id": "LGMnEd2vP-TT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEK5gxuW83zD"
      },
      "source": [
        "ds[0][0].shape"
      ],
      "id": "PEK5gxuW83zD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae4--Bm5UHBC"
      },
      "source": [
        "ds[0][1]"
      ],
      "id": "ae4--Bm5UHBC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coordinate-charity"
      },
      "source": [
        "## Losses"
      ],
      "id": "coordinate-charity"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIQQ6k7x9mIM"
      },
      "source": [
        "def one_step( xb,  yb, net,  criterion, optimizer, scheduler=None, mixup_proba=0.5, alpha=5, label_smoothing=True):\n",
        "  xb, yb = xb.to(DEVICE),yb.to(DEVICE)\n",
        "  optimizer.zero_grad()\n",
        "  if np.random.rand() < mixup_proba:\n",
        "        xb, y_a, y_b, _ = mixup_data(xb.cuda(), yb.cuda(), alpha=alpha)\n",
        "        yb = torch.clamp(y_a + y_b, 0, 1)\n",
        "  # if label_smoothing:\n",
        "  #     yb = smooth_label(yb)\n",
        "  o = net(xb)\n",
        "  loss = criterion(o, yb)\n",
        "  \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  o = o[\"logit\"]\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      l = loss.item()\n",
        "      o = o.sigmoid()\n",
        "      yb = (yb > 0.5)*1.0\n",
        "      lrap = label_ranking_average_precision_score(yb.cpu().numpy(), o.cpu().numpy())\n",
        "      o = (o > 0.5)*1.0\n",
        "      prec = (o*yb).sum()/(1e-6 + o.sum())\n",
        "      rec = (o*yb).sum()/(1e-6 + yb.sum())\n",
        "      f1 = 2*prec*rec/(1e-6+prec+rec)\n",
        "  return l, lrap, f1.item(), rec.item(), prec.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(net, criterion, val_laoder):\n",
        "    net.eval()\n",
        "\n",
        "    os, y = [], []\n",
        "    val_laoder = tqdm_notebook(val_laoder, leave = False, total=len(val_laoder))\n",
        "\n",
        "    for icount, (xb, yb) in  enumerate(val_laoder):\n",
        "        y.append(yb.to(DEVICE))\n",
        "        xb = xb.to(DEVICE)\n",
        "        o = net(xb)[\"logit\"]\n",
        "        os.append(o)\n",
        "    y = torch.cat(y)\n",
        "    o = torch.cat(os)\n",
        "    l = nn.BCEWithLogitsLoss()(o, y).item()\n",
        "    o = o.sigmoid()\n",
        "    y = (y > 0.5)*1.0\n",
        "    lrap = label_ranking_average_precision_score(y.cpu().numpy(), o.cpu().numpy())\n",
        "    o = (o > 0.5)*1.0\n",
        "    prec = ((o*y).sum()/(1e-6 + o.sum())).item()\n",
        "    rec = ((o*y).sum()/(1e-6 + y.sum())).item()\n",
        "    f1 = 2*prec*rec/(1e-6+prec+rec)\n",
        "    return l, lrap, f1, rec, prec,\n",
        "\n",
        "def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder, n=10):\n",
        "  net.train()\n",
        "  l, lrap, prec, rec, f1, icount = 0.,0.,0.,0., 0., 0\n",
        "  train_laoder = tqdm_notebook(train_laoder, leave = False)\n",
        "  epoch_bar = train_laoder\n",
        "  cnt = n \n",
        "  for (xb, yb) in  epoch_bar:\n",
        "      # epoch_bar.set_description(\"----|----|----|----|---->\")\n",
        "      cnt -= 1\n",
        "      _l, _lrap, _f1, _rec, _prec = one_step(xb, yb, net, criterion, optimizer)\n",
        "      l += _l\n",
        "      lrap += _lrap\n",
        "      f1 += _f1\n",
        "      rec += _rec\n",
        "      prec += _prec\n",
        "\n",
        "      icount += 1\n",
        "        \n",
        "      if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n",
        "          epoch_bar.set_postfix(\n",
        "            loss=\"{:.6f}\".format(l/icount),\n",
        "            lrap=\"{:.3f}\".format(lrap/icount),\n",
        "            prec=\"{:.3f}\".format(prec/icount),\n",
        "            rec=\"{:.3f}\".format(rec/icount),\n",
        "            f1=\"{:.3f}\".format(f1/icount),\n",
        "          )\n",
        "  l /= icount\n",
        "  lrap /= icount\n",
        "  f1 /= icount\n",
        "  rec /= icount\n",
        "  prec /= icount\n",
        "  \n",
        "  l_val, lrap_val, f1_val, rec_val, prec_val = evaluate(net, criterion, val_laoder)\n",
        "\n",
        "  scheduler.step()  \n",
        "  return (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val)\n",
        "\n",
        "def one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None, balance_sample=True):\n",
        "\n",
        "  save_root = Path(save_root) or MODEL_ROOT\n",
        "  saver = AutoSave(root=save_root, name=f\"birdclef_{model_name}_fold{fold}\", metric=\"f1_val\")\n",
        "  config_model =   {\"base_model_name\": \"efficientnet-b1\",\n",
        "    \"pretrained\": True,\n",
        "    \"num_classes\": 397}\n",
        "\n",
        "  # net =  EfficientNetSED(\"efficientnet-b1\", True, 397).to(DEVICE)\n",
        "  net =  ResNestSED(\"resnest50\", True, 397).to(DEVICE)\n",
        "  \n",
        "  #resnext_meta().to(DEVICE)\n",
        "  criterion = ImprovedPANNsLoss(weights=[1.0 , 0.5])\n",
        "  optimizer = optim.AdamW(net.parameters(), lr=CFG.lr)\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,  T_max=50)\n",
        "  train_data = BirdClefDataset_V2( meta=train_set, sr=SR, duration=DURATION, is_train=True)\n",
        "  if balance_sample:\n",
        "    all_targets = []\n",
        "    for i in range(len(train_set)):\n",
        "      ebird_code = train_set.iloc[i][\"primary_label\"]\n",
        "      labels = np.zeros(397, dtype=\"f\")\n",
        "      labels[CFG.target_columns.index(ebird_code)] = 1\n",
        "      all_targets.append(labels)\n",
        "    all_targets = np.array(all_targets)\n",
        "    train_laoder = DataLoader(train_data, batch_size=CFG.batch_size, num_workers=CFG.num_workers, sampler=SimpleBalanceClassSampler(all_targets, 397), pin_memory=True)\n",
        "  else:\n",
        "    train_laoder = DataLoader(train_data, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=True, pin_memory=True)\n",
        "  val_data = BirdClefDataset_V2( meta=val_set,  sr=SR, duration=DURATION, is_train=False)\n",
        "  val_laoder = DataLoader(val_data, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=False, pin_memory=True)\n",
        "  epochs_bar = tqdm(list(range(epochs)), leave=False)\n",
        "  for epoch  in epochs_bar:\n",
        "    epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n",
        "    net.train()\n",
        "    (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val) = one_epoch(\n",
        "        net=net,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_laoder=train_laoder,\n",
        "        val_laoder=val_laoder,\n",
        "      )\n",
        "    epochs_bar.set_postfix(\n",
        "    loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n",
        "    prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n",
        "    rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n",
        "    f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n",
        "    lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n",
        "    )\n",
        "    print(\n",
        "        \"[{epoch:02d}] loss: {loss} lrap: {lrap} f1: {f1} rec: {rec} prec: {prec}\".format(\n",
        "            epoch=epoch,\n",
        "            loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n",
        "            prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n",
        "            rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n",
        "            f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n",
        "            lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n",
        "        )\n",
        "    )\n",
        "    if save:\n",
        "      metrics = {\n",
        "          \"loss\": l, \"lrap\": lrap, \"f1\": f1, \"rec\": rec, \"prec\": prec,\n",
        "          \"loss_val\": l_val, \"lrap_val\": lrap_val, \"f1_val\": f1_val, \"rec_val\": rec_val, \"prec_val\": prec_val,\n",
        "          \"epoch\": epoch,\n",
        "      }\n",
        "      saver.log(net, metrics)\n",
        "  torch.save(net.state_dict(), save_root/f\"last_epochs_fold{fold}.pth\")"
      ],
      "id": "FIQQ6k7x9mIM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSfBGhxxKnK0"
      },
      "source": [
        "def train(model_name, epochs=20, save=True, n_splits=5, seed=177, save_root=None, suffix=\"\"):\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  # environment\n",
        "  set_seed(CFG.seed)\n",
        "  device = get_device()\n",
        "  # validation\n",
        "  # data\n",
        "  save_root.mkdir(exist_ok=True, parents=True)\n",
        "  for i in range(5):\n",
        "    if i not in CFG.folds:\n",
        "        continue\n",
        "    save_root = MODEL_ROOT/f\"fold-{i}\"\n",
        "    save_root.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    print(\"=\" * 120)\n",
        "    print(f\"Fold {i} Training\")\n",
        "    print(\"=\" * 120)\n",
        "    trn_df = df[df['fold']!=i].reset_index(drop=True)\n",
        "    val_df = df[df['fold']==i].reset_index(drop=True)\n",
        "    one_fold(model_name, fold=i, train_set=trn_df , val_set=val_df , epochs=CFG.epochs, save=save, save_root=save_root)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "id": "mSfBGhxxKnK0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTapWKPDKp5Y"
      },
      "source": [
        "\n",
        "for model_name in MODEL_NAMES:\n",
        "  print(\"\\n\\n###########################################\", model_name.upper())\n",
        "  try:\n",
        "    train(model_name, epochs=35, save_root=MODEL_ROOT, suffix=f\"_sr{CFG.sample_rate}_d{CFG.duration}_v1_v1\")\n",
        "  except Exception as e:\n",
        "    # print(f\"Error {model_name} : \\n{e}\")\n",
        "    raise ValueError() from  e"
      ],
      "id": "xTapWKPDKp5Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk9Hz7vKvwGR"
      },
      "source": [
        "!curl https://download.pytorch.org/models/resnet50-19c8e357.pth"
      ],
      "id": "gk9Hz7vKvwGR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io4OdQSqmHZY"
      },
      "source": [
        "## Stage 2 - Training with soundcape"
      ],
      "id": "Io4OdQSqmHZY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq9sm3FlNmeb"
      },
      "source": [
        "location_map = {\"COL\": [5.57 , -75.85, \"2019-10-16\"], \"COR\":[10.12, -84.51, \"2019-09-20\"], \\\n",
        "                \"SNE\": [38.49, -119.95, \"2018-05-17\"], \"SSW\":[42.47, -76.45, \"2017-05-30\"]}\n",
        "\n",
        "DATADIR = Path(\"/content/train_soundscapes/\")\n",
        "train_soundscape = pd.read_csv(\"/content/train_soundscapes.csv\")\n",
        "\n",
        "all_audios = list(DATADIR.glob(\"*.ogg\"))\n",
        "list_file = [\"_\".join(str(i).split('/')[-1].split(\"_\")[:2]) for i in all_audios]\n",
        "list_file = [str(i).split('/')[-1] for i in all_audios]\n",
        "def get_data(t):\n",
        "    startstr = \"_\".join(t.split(\"_\")[:2])\n",
        "    date = \"\"\n",
        "    for filepath in list_file:\n",
        "        if filepath.startswith(startstr):\n",
        "            date = str(filepath.split(\"_\")[2][:-4])\n",
        "            break\n",
        "    if date == \"\":\n",
        "        date = location_map[str(t).split(\"_\")[1]][2]\n",
        "    else:\n",
        "        date = date[:4] + '-' + date[4:6] + '-' + date[6:]\n",
        "    return date\n",
        "def preprocess_data(test_df):\n",
        "    test_df['latitude'] = test_df['row_id'].apply(lambda t: location_map[str(t).split(\"_\")[1]][0])\n",
        "    test_df['longitude'] = test_df['row_id'].apply(lambda t: location_map[str(t).split(\"_\")[1]][1])\n",
        "    test_df['date'] = test_df['row_id'].apply(lambda t: location_map[str(t).split(\"_\")[1]][2])\n",
        "    return test_df\n",
        "train_soundscape = preprocess_data(train_soundscape)"
      ],
      "id": "Jq9sm3FlNmeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ25fQz2uHzH"
      },
      "source": [
        "class SoundscapeDataset(torchdata.Dataset):\n",
        "    def __init__(self,\n",
        "                 df: pd.DataFrame,\n",
        "                 datadir: Path,\n",
        "                 img_size=224,\n",
        "                 waveform_transforms=None,\n",
        "                 period=20,\n",
        "                 validation=False):\n",
        "        self.df = df\n",
        "        self.datadir = datadir\n",
        "        self.img_size = img_size\n",
        "        self.waveform_transforms = waveform_transforms\n",
        "        self.period = period\n",
        "        self.validation = validation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        sample = self.df.loc[idx, :]\n",
        "        wav_name = sample[\"filename\"]\n",
        "        ebird_code = sample[\"birds\"]\n",
        "        meta_data = np.array(sample[[\"latitude\", \"longitude\"]])\n",
        "        data = np.array(list(map(int, sample[\"date\"].split(\"-\"))))\n",
        "        meta_data = np.hstack((meta_data, data)).astype(np.float32)\n",
        "        y, sr = sf.read(self.datadir / ebird_code / wav_name)\n",
        "        start = sr*(sample['seconds']-5)\n",
        "        end =  sr*sample['seconds']\n",
        "        y = y[start:end].astype(np.float32)\n",
        "        y = np.nan_to_num(y)\n",
        "        if self.waveform_transforms:\n",
        "            y = self.waveform_transforms(y)\n",
        "        y = np.nan_to_num(y)\n",
        "        if ebird_code != 'nocall':\n",
        "          labels = np.zeros(len(CFG.target_columns), dtype=float)\n",
        "          labels[CFG.target_columns.index(ebird_code)] = 1.0\n",
        "        return  y, meta_data, labels"
      ],
      "id": "MZ25fQz2uHzH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkXnHLnp0JfT"
      },
      "source": [
        "def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder):\n",
        "  net.train()\n",
        "  l, lrap, prec, rec, f1, icount = 0.,0.,0.,0., 0., 0\n",
        "  train_laoder = tqdm_notebook(train_laoder, leave = False)\n",
        "  epoch_bar = train_laoder\n",
        "  \n",
        "  for (xb,meta, yb) in  epoch_bar:\n",
        "      # epoch_bar.set_description(\"----|----|----|----|---->\")\n",
        "      _l, _lrap, _f1, _rec, _prec = one_step(xb,meta, yb, net, criterion, optimizer)\n",
        "      l += _l\n",
        "      lrap += _lrap\n",
        "      f1 += _f1\n",
        "      rec += _rec\n",
        "      prec += _prec\n",
        "\n",
        "      icount += 1\n",
        "        \n",
        "      if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n",
        "          epoch_bar.set_postfix(\n",
        "            loss=\"{:.6f}\".format(l/icount),\n",
        "            lrap=\"{:.3f}\".format(lrap/icount),\n",
        "            prec=\"{:.3f}\".format(prec/icount),\n",
        "            rec=\"{:.3f}\".format(rec/icount),\n",
        "            f1=\"{:.3f}\".format(f1/icount),\n",
        "          )\n",
        "  \n",
        "  scheduler.step()\n",
        "\n",
        "  l /= icount\n",
        "  lrap /= icount\n",
        "  f1 /= icount\n",
        "  rec /= icount\n",
        "  prec /= icount\n",
        "  # l_val, lrap_val, f1_val, rec_val, prec_val = evaluate(net, criterion, val_laoder)\n",
        "  return l, lrap, f1, rec, prec"
      ],
      "id": "NkXnHLnp0JfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvFCB0bWyWe9"
      },
      "source": [
        "def one_fold(model_name, train_set,, epochs=20, save=True, save_root=None):\n",
        "\n",
        "  save_root = Path(save_root) or MODEL_ROOT\n",
        "  saver = AutoSave(root=save_root, name=f\"birdclef_{model_name}_fold{fold}\", metric=\"f1_val\")\n",
        "  net = TimmSED(\n",
        "        base_model_name=CFG.base_model_name,\n",
        "        pretrained=CFG.pretrained,\n",
        "        num_classes=CFG.num_classes,\n",
        "        in_channels=CFG.in_channels).to(DEVICE)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  # optimizer = optim.Adam(net.parameters(), lr=0.004)\n",
        "  base_optim_param = {'lr':0.004}\n",
        "  base_optim = Ralamb(net.parameters(), **base_optim_param)\n",
        "  optim_param = {'k':5, 'alpha':0.5}\n",
        "  optimizer = Lookahead(base_optim, **optim_param)\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=epochs)\n",
        "  train_laoder = torchdata.DataLoader(\n",
        "            SoundscapeDataset(\n",
        "                train_set,\n",
        "                CFG.train_datadir,\n",
        "                img_size=CFG.img_size,\n",
        "               \n",
        "                period=CFG.period,\n",
        "                validation=False\u001c\n",
        "            ), **CFG.loader_params['train']) \n",
        "  epochs_bar = tqdm(list(range(epochs)), leave=False)\n",
        "  for epoch  in epochs_bar:\n",
        "    epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n",
        "    net.train()\n",
        "    l, lrap, f1, rec, prec = one_epoch(\n",
        "        net=net,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_laoder=train_laoder\n",
        "      )\n",
        "    print(\n",
        "        \"[{epoch:02d}] loss: {loss} lrap: {lrap} f1: {f1} rec: {rec} prec: {prec}\".format(\n",
        "            epoch=epoch,\n",
        "            loss=\"({:.6f}, )\".format(l),\n",
        "            prec=\"({:.3f}, )\".format(prec),\n",
        "            rec=\"({:.3f},)\".format(rec),\n",
        "            f1=\"({:.3f}, )\".format(f1),\n",
        "            lrap=\"({:.3f},)\".format(lrap),\n",
        "        )\n",
        "    )\n",
        "    if save:\n",
        "      metrics = {\n",
        "          \"loss\": l, \"lrap\": lrap, \"f1\": f1, \"rec\": rec, \"prec\": prec,\n",
        "          \"epoch\": epoch,\n",
        "      }\n",
        "      saver.log(net, metrics)"
      ],
      "id": "SvFCB0bWyWe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W72qYUV5x45J"
      },
      "source": [
        "## train 1 epochs\n",
        "PATH = [\"\"]\n",
        "for model_name in MODEL_NAMES:\n",
        "  one_fold(model_name, train_set, epochs=1, save=True, save_root=None):\n"
      ],
      "id": "W72qYUV5x45J",
      "execution_count": null,
      "outputs": []
    }
  ]
}